# shortcut to this dotfiles path is $ZSH
export ZSH=$HOME/.dotfiles-zsh

# your project folder that we can `c [tab]` to
export PROJECTS=~/dev

# Stash your environment variables in ~/.localrc. This means they'll stay out
# of your main dotfiles repository (which may be public, like this one), but
# you'll have access to them in your scripts.
if [[ -a ~/.localrc ]]
then
  source ~/.localrc
fi

# all of our zsh files
typeset -U config_files
config_files=($ZSH/**/*.zsh)

# load the path files
for file in ${(M)config_files:#*/path.zsh}
do
  source $file
done

# load everything but the path and completion files
for file in ${${config_files:#*/path.zsh}:#*/completion.zsh}
do
  source $file
done

# initialize autocomplete here, otherwise functions won't be loaded
autoload -U compinit
compinit

# load every completion after autocomplete loads
for file in ${(M)config_files:#*/completion.zsh}
do
  source $file
done

unset config_files

# Better history
# Credits to https://coderwall.com/p/jpj_6q/zsh-better-history-searching-with-arrow-keys
autoload -U up-line-or-beginning-search
autoload -U down-line-or-beginning-search
zle -N up-line-or-beginning-search
zle -N down-line-or-beginning-search
bindkey "^[[A" up-line-or-beginning-search # Up
bindkey "^[[B" down-line-or-beginning-search # Down

# AWS
export AWS_USERNAME="david.jhinku"
alias ldl="login_data_lake"
alias lmd="login_mask_data"

function aws_mfa(){
  unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN
  local token="$1"

  echo -n "MFA Token: "
  read token

  local result=$(aws sts get-session-token --serial-number arn:aws:iam::030664766007:mfa/$AWS_USERNAME --token-code "$token" --duration-seconds 43200)
  export AWS_ACCESS_KEY_ID=$(echo "$result" | jq -r '.Credentials.AccessKeyId')
  export AWS_SECRET_ACCESS_KEY=$(echo "$result" | jq -r '.Credentials.SecretAccessKey')
  export AWS_SESSION_TOKEN=$(echo "$result" | jq -r '.Credentials.SessionToken')
}

function login_data_lake() {
    if [ -z "$AWS_SESSION_TOKEN" ]; then
       aws_mfa;
    fi;

    local result=$(aws redshift get-cluster-credentials --cluster-identifier data-lake \
                      --db-user $AWS_USERNAME --db-name dev --duration-seconds 3600 --auto-create --db-groups payments)

	export PGUSER=$(echo "$result" | jq -r '.DbUser')
	export PGPASSWORD=$(echo "$result" | jq -r '.DbPassword')

    echo 'You can connect via if on the VPN: `pgcli -h data-lake.sezzle.internal -p 5439 -U "$DB_USER" dev`'
    echo 'You can view your temporary password via `echo $PGPASSWORD`'
}

function login_mask_data() {
    if [ -z "$AWS_SESSION_TOKEN" ]; then
       aws_mfa;
    fi;

    local result=$(aws redshift get-cluster-credentials --cluster-identifier data-lake --db-user $AWS_USERNAME --db-name staging --duration-seconds 3600 --auto-create --db-groups analysis)

    export MASK_USER=$(echo "$result" | jq -r '.DbUser')
    export MASK_PASS=$(echo "$result" | jq -r '.DbPassword')
    export MASK_HOST="data-lake.sezzle.internal"
    export MASK_SCHEMA=$(echo $AWS_USERNAME|sed "s/\./_/g")
	
    echo 'You can connect via if on the VPN: `pgcli -h data-lake.sezzle.internal -p 5439 -U "$MASK_USER" staging`'
    echo 'You can view your temporary password via `echo $MASK_PASS`'
}